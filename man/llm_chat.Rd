% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/llm_chat.R
\name{llm_chat}
\alias{llm_chat}
\title{Chat with LLM}
\usage{
llm_chat(
  prompt,
  model = default_chat_model(),
  system_prompt = NULL,
  format = NULL
)
}
\arguments{
\item{prompt}{Character string with the prompt}

\item{model}{Character string specifying the model to use}

\item{system_prompt}{Character string with the system prompt}

\item{format}{Optional list specifying the JSON schema for structured output.
See https://ollama.com/blog/structured-outputs for details and examples.}
}
\value{
If format is specified, returns a parsed R object.
Otherwise, returns a character string containing the model's response.
}
\description{
Send a prompt to the LLM and get a response.
}
\examples{
# Free text response
llm_chat(prompt = "What is 2 + 2?")

# Structured output using JSON schema
llm_chat(
  prompt = "Extract the value and unit: 42 kilometers",
  format = list(
    type = "object",
    properties = list(
      value = list(type = "number"),
      unit = list(type = "string")
    ),
    required = c("value", "unit")
  )
)
}
