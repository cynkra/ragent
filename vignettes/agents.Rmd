---
title: "Playing with AI Agents in R"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Playing with AI Agents in R}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Intro

Crazy months have passed since I've first started to [play with local LLMs](https://blog.cynkra.com/posts/2024-07-27-llama3.1/).

With llama 3.2 and 3.3, we got fantastic small and large models that run on consumer hardware.
With deepseek R1, we got some ridicoulously good MIT-licensed models that are competitive with the best Models from OpenAI.

And now, we got a shiny fully-speced M4 Mac Mini for our office that runs all of this like a charm.

It's local LLM time! ðŸ¥³

Let's explore how we can use of this to run agents in R. I created a simple toy package to demonstrate some key concepts of AI agents in R.

## What is an agent?

Agents are chats that can do stuff. And R is a wonderful language to tell LLMs what to do.

Think about it this way:
1. You tell them what you want in plain English
2. They figure out what tools they need
3. They get the job done and explain what they did

## R and Ollama

For my experiments with agents, I'm using plain R and ollama.
I know that there is the R package `ellmer`, and you can do many thinks that I am doint here with it.

However, I found that `ellmer` is confuscating the working of tools within agents.

By talking directly to the ollama API, we see exactly what they are doing:

- How tools are defined
- What structured output looks like
- How the whole conversation flows

I will use diagrams to show the conversation flow and the tools that are used.


## A simple chat

Getting started is straightforward. First, let's see what we can do with plain chat:

```{r, eval = FALSE}
library(ragent)

# Let's chat!
llm_chat("What is the capital of France?")
#> Paris is the capital of France.
```

No surprise here. It's just a chat.

```{r simple-chat, echo=FALSE, message=FALSE}
library(DiagrammeR)
mermaid("
sequenceDiagram
    participant User
    participant LLM

    User->>LLM: Question
    Note over LLM: No tools needed
    LLM-->>User: Direct response
")
```

## A simple agent

Here comes the good part! Let's turn our LLM into a calculator that actually understands what we're asking:

```{r, eval = FALSE}
# Let's start easy
agent_calculator("What is 15% of 850?")
#> 15% of 850 equals 127.50.
#> This is calculated as 850 * 0.15.

# Time for pizza math!
agent_calculator("If I have 3 pizzas and each is cut into 8 slices,
                 and 5 people eat 4 slices each, how many slices remain?")
#> Let's solve this step by step:
#> 1. Total slices = 3 pizzas * 8 slices = 24 slices
#> 2. Eaten slices = 5 people * 4 slices = 20 slices
#> 3. Remaining slices = 24 - 20 = 4 slices
#> Therefore, 4 slices remain.
```

```{r calculator-flow, echo=FALSE, message=FALSE}
library(DiagrammeR)
mermaid("
sequenceDiagram
    participant User
    participant Agent
    participant Calculator Tool

    User->>Agent: Math question
    Note over Agent: Analyzes question<br/>Decides to use calculator
    Agent->>Calculator Tool: Structured calculation request
    Calculator Tool-->>Agent: Numerical result
    Note over Agent: Formats response
    Agent-->>User: Explained calculation
")
```

Cool, right? Our agent:
- Takes plain English questions
- Does the math for us
- Shows exactly how it got there

## Something useful

Now here's where it gets really interesting. Let's build a RAG agent - think of it as a smart librarian for your docs:

```{r, eval = FALSE}
# First, let's feed it some knowledge
writeLines(
  "The tidyverse is a collection of R packages designed for data science.
   Core packages include ggplot2 for visualization, dplyr for data manipulation,
   and tidyr for data tidying. All packages share common design principles.",
  "docs/tidyverse.md"
)

# Time to test its memory
agent_rag("What is ggplot2 used for?", dir = "docs/")
#> Based on the documentation:
#> ggplot2 is a core package of the tidyverse used for visualization.
#> [Source: tidyverse.md]
```

```{r rag-flow, echo=FALSE, message=FALSE}
library(DiagrammeR)
mermaid("
sequenceDiagram
    participant User
    participant Agent
    participant RAG Tool
    participant Documents

    User->>Agent: Question about docs
    Note over Agent: Analyzes question<br/>Decides to use RAG
    Agent->>RAG Tool: Search request
    RAG Tool->>Documents: Search in directory
    Documents-->>RAG Tool: Relevant passages
    RAG Tool-->>Agent: Context + passages
    Note over Agent: Synthesizes answer<br/>with citations
    Agent-->>User: Contextualized response
")
```

Look at that! Our agent:
- Reads through the docs
- Picks out exactly what we need
- Even cites its sources (such a good student!)

## Conclusion


It's a sweet setup for tinkering:
- We've got R doing what R does best
- Ollama keeping things local and snappy
- Everything visible and hackable
- No magic, just code we can understand

## Next steps

After all this playing around, what have we learned?

1. **This Actually Works!**
   - These agents really understand us
   - They can use real tools
   - They explain everything they do

2. **It's Not Rocket Science**
   - Break down the big stuff
   - Pick the right tools
   - Keep track of what's happening

3. **Time to Experiment**
   - All the code is there to play with
   - You can see every moving part
   - Perfect for tinkering

Want to join the fun?
- Try your own tools
- Add whatever features you dream up
- Build something cool

The best part? It's all running right there on your machine. No cloud needed, no API keys to manage - just pure local LLM goodness! ðŸš€

```{r, include = FALSE}
# Clean up demo files
unlink("docs", recursive = TRUE)
```