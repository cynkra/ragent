---
title: "Introduction to ragent"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to ragent}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

The `ragent` package provides a simple way to create and interact with AI agents using local LLMs through Ollama. This vignette demonstrates the basic usage of the package.

## Prerequisites

Before using `ragent`, make sure you have:

1. [Ollama](https://ollama.ai/) installed and running on your system
2. Required models pulled:
   - llama2-3b:1b (for simple tasks)
   - deepseek-r1:8b (for complex reasoning)
   - nomic-embed-text (for knowledge base embeddings)

You can pull these models using:
```bash
ollama pull llama2-3b:1b
ollama pull deepseek-r1:8b
ollama pull nomic-embed-text
```

## Basic Usage

First, let's load the package and create a simple agent:

```{r setup}
library(ragent)

# Create a basic agent (uses llama2-3b:1b by default)
agent <- create_agent()
```

The agent automatically selects the appropriate model based on the query:
- Simple queries use llama2-3b:1b
- Complex reasoning queries use deepseek-r1:8b

### Simple Questions

For simple questions, the agent uses the lighter llama2-3b:1b model:

```{r eval=FALSE}
# Uses llama2-3b:1b
agent$ask("What is the capital of France?")
```

### Complex Reasoning

For questions requiring more reasoning, the agent automatically switches to deepseek-r1:8b:

```{r eval=FALSE}
# Uses deepseek-r1:8b
agent$ask("Can you explain why Paris became the capital of France and its historical significance?")
```

### Using Tools

The agent becomes more powerful when equipped with tools. For tool-based tasks, it always uses the more capable deepseek-r1:8b model:

```{r}
tools <- list(
  calculator = calc_tool
)

agent_with_tools <- create_agent(
  tools = tools,
  system_prompt = "You are a helpful assistant specialized in calculations."
)
```

Now we can ask the agent to perform calculations:

```{r eval=FALSE}
agent_with_tools$process_task("Calculate the square root of 21432.13213")
```

### Knowledge Base Tool

The knowledge base tool is a powerful feature that enables semantic search over your local documentation using vector embeddings. It works by:

1. **Document Processing**:
   - Reads markdown and text files from a specified directory
   - Splits documents into meaningful chunks (e.g., by paragraphs)
   - Maintains source information for each chunk

2. **Vector Embeddings**:
   - Uses the nomic-embed-text model to create embeddings
   - Embeddings capture the semantic meaning of text
   - Enables similarity-based search beyond simple keyword matching

3. **Efficient Storage**:
   - Stores embeddings in a local vector store (RDS file)
   - Automatically updates when documents change
   - Optimized for quick similarity searches

4. **Smart Search**:
   - Finds the most semantically relevant chunks for each query
   - Returns multiple context chunks with similarity scores
   - Preserves source information for traceability

Let's look at some examples:

```{r eval=FALSE}
# Create a knowledge base tool
knowledgebase <- kb_tool(
  dir = "~/knowledgebase",  # your existing knowledge base
  n_context = 3             # number of similar chunks to return
)

# Use the tool directly
knowledgebase("How do I submit a receipt?")

# Try different queries
knowledgebase("What are our policies?")
knowledgebase("What are the office hours?")

# You can also update the knowledge base when files change
# update_vector_store("~/knowledgebase")
```

The tool returns the most relevant chunks from your knowledge base, including source information and similarity scores. This is useful when you want to:
- Quickly search through documentation
- Find specific policies or procedures
- Look up information across multiple files
- Get context-aware answers to questions

The tool can be used in two ways:

1. **Standalone Usage**:
   ```r
   # Direct usage
   knowledgebase("What are our vacation policies?")
   ```

2. **As Part of an Agent**:
   ```r
   # Create an agent that can intelligently query the knowledge base
   agent <- create_agent(
     tools = list(knowledgebase = knowledgebase),
     system_prompt = "You are a helpful assistant with access to a knowledge base.
       When answering questions:
       1. Break down complex queries into smaller, focused lookups
       2. Make multiple queries to gather comprehensive information
       3. Combine and synthesize information from different sources
       4. Ask follow-up queries based on initial results
       Always cite your sources when providing information."
   )

   # The agent will make multiple intelligent queries
   agent$process_task(
     "I'm planning my vacation. What do I need to know about our policies,
      procedures, and any deadlines?"
   )

   # The agent might make queries like:
   # - "vacation policy"
   # - "time off request procedure"
   # - "vacation approval process"
   # - "vacation notice period"
   # - "holiday calendar"

   # Another example with multiple aspects
   agent$process_task(
     "I need to organize a team meeting and order lunch.
      What are the relevant policies and procedures?"
   )

   # The agent might query:
   # - "meeting room booking"
   # - "catering policy"
   # - "expense reimbursement"
   # - "dietary restrictions policy"
   ```

## Model Selection

The agent uses a simple heuristic to determine which model to use:

1. Simple queries (llama2-3b:1b):
   - Basic facts
   - Short answers
   - Direct questions

2. Complex queries (deepseek-r1:8b):
   - Questions starting with why, how, explain
   - Analysis requests
   - Comparisons
   - Tool-based tasks

You can also specify different models when creating the agent:

```{r eval=FALSE}
agent <- create_agent(
  model = "llama2-3b:1b",        # for simple tasks
  reasoning_model = "deepseek-r1:8b"  # for complex tasks
)
```

## Conclusion

The `ragent` package provides a flexible framework for creating AI agents that can use local LLMs and custom tools. By using different models for different types of tasks, it balances efficiency with capability - using a lighter model for simple tasks and a more powerful model for complex reasoning.